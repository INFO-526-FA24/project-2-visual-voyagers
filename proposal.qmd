---
title: "Analysis of different classification models"
subtitle: "Proposal"
author: 
      - name: "Visual Voyagers"
affiliations:
      - name: "College of Information Science, University of Arizona" 
format:
  html:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    embed-resources: true
editor: visual
code-annotations: hover
execute:
  warning: false      
---

## High level goal

To visualize different Machine Learning Classification Algorithms which can help identify whether a particular tumor is malignant or benign.

## Dataset

```{r}
#| label: load-dataset
#| message: false
```

Source: The data set we choose for this project is "Breast Cancer Diagnostic Data" we are getting the data set from UCI Machine Learning Repository: <https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29>

The source of dataset <https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data>

## **Why this Data set?**

The **Breast Cancer Wisconsin Diagnostic Dataset** is a widely used, high-quality dataset containing **569 samples** with **30 features** that describe tumor characteristics, allowing for effective **benign vs. malignant** binary classification. Its clinical relevance, well-labeled structure, and feature variety make it ideal for developing and benchmarking diagnostic models in machine learning.

This dataset is characterized by its **low dimensionality**, making it particularly well-suited for testing various classification algorithms and benchmarking their performance. It provides a reliable foundation for researchers and students to build predictive models, contributing to advancements in medical diagnostics and offering a valuable educational tool.

## Basic EDA

```{r}
library(ggplot2)
library(dplyr)

your_dataset <- read.csv("data/cancer_dataset.csv")

# View the first few rows of the dataset
head(your_dataset)

# Get a summary of the dataset
summary(your_dataset)

# Check the structure of the dataset
str(your_dataset)

# Check for missing values
sum(is.na(your_dataset))

# Descriptive statistics
describe <- your_dataset %>% 
  summarise_all(list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE)))
print(describe)
num_rows <- nrow(your_dataset)
cat("Number of rows:", num_rows, "\n")
num_cols <- ncol(your_dataset)
cat("Number of columns:", num_cols, "\n")
unique_values <- sapply(your_dataset, function(x) length(unique(x)))
cat("Unique values per column:\n")
print(unique_values)
```

## Questions

**Question 1**: How does the accuracy of Model A (e.g., Logistic Regression) compare to Model B (e.g., SVM) when predicting the diagnosis? Which model performs best on the test set?

##### why this question ?

We choose this question because comparing the accuracy of different models, like Logistic Regression and SVM, is essential for identifying the best-performing model on the test set. This question helps you understand which model is most effective for predicting the diagnosis in this dataset.

**Question 2**:How do the Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) scores differ among the models? This can show which model offers the best trade-off between true positive and false positive rates.

##### why this question ?

This question is valuable because comparing ROC curves and AUC scores provides insight into each model’s ability to balance true positives and false positives. It helps identify the model with the best overall performance, especially when handling imbalanced data or cases where both sensitivity and specificity are important.

**Question 3**:How does tuning specific hyperparameters (e.g., regularization in Logistic Regression, kernel type in SVM) affect model performance?

##### why this question ?

This question is important because tuning hyperparameters allows you to optimize each model’s performance, potentially improving accuracy, precision, and recall. It helps you understand how specific parameter choices impact model effectiveness on this dataset.

**Question 4**:How robust the models is to noise and outliers?

##### why this question ?

This question is crucial because assessing robustness to noise and outliers reveals each model’s stability and reliability under real-world conditions. It helps determine which models are better suited for handling unpredictable or messy data.

## Plan of Attack

| Task Name                             | Status    | Assignee                        | Due        | Priority | Summary                                         |
|------------|------------|------------|------------|------------|----------------|
| Create Proposal                       | Completed | Gaurangi, Tushar                | 11/12/24   | High     | Finish proposal and upload to GitHub repo       |
| Feature Engineering                   | Completed | Viren, Bhaskar                  | 11/12/24   | moderate | work on basic EDA                               |
| Update the proposal after peer review | WIP       | Gaurangi, Tushar,Viren, Bhaskar | 11/18/24   | high     | Revise proposal according to the feedback       |
| Question 1                            | WIP       | Gaurangi, Tushar                | 11/19/2024 | High     | Finish Q1                                       |
| Question 2                            | WIP       | Viren Bhaskar                   | 11/22/2024 | High     | Finish Q2                                       |
| Question 3                            | WIP       | Bhaskar, Tushar                 | 11/23/2024 | High     | Finish Q3                                       |
| Question 4                            | WIP       | Viren, Gaurangi                 | 11/27/2024 | High     | Finish Q4                                       |
| Final analysis                        | WIP       | Team                            | 12/01/24   | High     | Finish design poster for ishowcase presentation |
| Poster review                         | WIP       | Team                            | 12/04/24   | High     | Do peer review in class                         |
| Final ishowcase presentation          | WIP       | Team                            | 12/11/24   | High     | Do ishowcase presentation                       |
| Write-up                              | WIP       | Team                            | 12/13/24   | High     | Finish final write up                           |

#### Final repository organisation

-   **data/**: Contains dataset. It includes README, which details dataset parameters.

-   **plots/**:Includes ggplot2 visualizations for each question. The README file explains each plot.

-   **docs/**:Consist of write-up, final report. README outlines project goals and methods.

-   **proposal.qmd**: Project proposal in qmd format.
